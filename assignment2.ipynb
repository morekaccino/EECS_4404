{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "def fit_NeuralNetwork(X_train, y_train, alpha, hidden_layer_sizes, epochs):\n",
    "    layer_units = ([len(X_train[-1])] + hidden_layer_sizes + [1])\n",
    "    W = [np.random.rand(n_fan_in_ + 1, n_fan_out_) for n_fan_in_, n_fan_out_ in\n",
    "         zip(layer_units[:-1], layer_units[1:])]\n",
    "    # W = np.true_divide(W, 10)\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "    error_list = []\n",
    "    error_dic = {}\n",
    "    for _ in range(epochs):\n",
    "        error_over_epoch = 0\n",
    "        np.random.shuffle(X_train)\n",
    "        for N, x_n in enumerate(X_train):\n",
    "            X_n, S_n = forwardPropagation(x_n, W)\n",
    "            g_n = backPropagation(X_n, y_train[N], S_n, W)\n",
    "            W = updateWeights(W, g_n, alpha)\n",
    "            # error_over_epoch += errorPerSample(X_n, y_train[N])\n",
    "            error_over_epoch += 1 if pred(x_n, W) != y_train[N] else 0\n",
    "        error_list.append(error_over_epoch / len(X_train))\n",
    "        error_dic[error_over_epoch] = W\n",
    "    print(error_list)\n",
    "    return error_list, W\n",
    "\n",
    "\n",
    "def forwardPropagation(x, weights):\n",
    "    Xl = np.array(x)\n",
    "    W = np.array(weights)\n",
    "    S = []\n",
    "    X = [x]\n",
    "    for index, l in enumerate(W):\n",
    "        wl = np.array(l)\n",
    "        sl = np.transpose(wl).dot(Xl)\n",
    "        Xl_before_activation = sl\n",
    "        if index != len(W) - 1:\n",
    "            activation_function = np.vectorize(activation)\n",
    "            Xl = activation_function(Xl_before_activation)\n",
    "            Xl = np.insert(Xl, 0, 1, axis=0)\n",
    "        else:\n",
    "            output_function = np.vectorize(outputf)\n",
    "            Xl = output_function(Xl_before_activation)\n",
    "        X.append(Xl)\n",
    "        S.append(sl)\n",
    "    np.delete\n",
    "    return np.array(X), np.array(S)\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "def backPropagation(X, y_n, s, weights):\n",
    "    weights_copy = deepcopy(weights)\n",
    "    g = [None] * len(X)\n",
    "    X = np.array(X)\n",
    "    for layer, Xl in enumerate(reversed(X)):\n",
    "        layer = len(X) - layer - 1\n",
    "        if layer == len(X) - 1:\n",
    "            delta = 2 * (Xl[0] - y_n) * derivativeOutput(s[-1][0])\n",
    "            g[layer] = np.array([delta])\n",
    "        elif layer > 0:\n",
    "            derivatives = np.zeros([len(Xl) - 1, len(Xl) - 1])\n",
    "            for i in range(len(Xl) - 1):\n",
    "                derivatives[i][i] = derivativeActivation(Xl[i + 1])\n",
    "\n",
    "            Wl = weights_copy[layer]\n",
    "            # Wl = weights[layer]\n",
    "            Wl_t = np.array(Wl)\n",
    "            g[layer] = np.dot(np.dot(Wl_t, (g[layer + 1]).T)[1:].T, derivatives)\n",
    "            # g[layer] = ((Wl_t).dot((g[layer + 1]).T)[1:]).T.dot(derivatives)\n",
    "\n",
    "    g = g[1:]\n",
    "\n",
    "    updatedW = weights_copy\n",
    "    # updatedW = weights\n",
    "    for layer, Xl in enumerate(X[:-1]):\n",
    "        updatedW[layer] = np.dot(np.array([Xl]).T, np.array([g[layer]]))\n",
    "\n",
    "    return (updatedW)\n",
    "\n",
    "\n",
    "def updateWeights(weights, g, alpha):\n",
    "    return np.subtract(np.array(weights), np.multiply(np.array(g), alpha))\n",
    "\n",
    "\n",
    "def activation(s):\n",
    "    return 0 if s <= 0 else s\n",
    "    # return np.tanh(s)\n",
    "\n",
    "\n",
    "def derivativeActivation(s):\n",
    "    return 0 if s <= 0 else 1\n",
    "    # return (1 - np.tanh(s) ** 2)\n",
    "\n",
    "\n",
    "def outputf(s):\n",
    "    return (1) / ((1) + np.exp(-s))\n",
    "\n",
    "\n",
    "def derivativeOutput(s):\n",
    "    # return (outputf(s)) * (1 - outputf(s))\n",
    "    return (np.exp(-s)) / ((1 + np.exp(-s)) ** 2)\n",
    "\n",
    "\n",
    "def errorf(x_L, y):\n",
    "    if y == 1:\n",
    "        return np.log(x_L)\n",
    "    else:\n",
    "        return -np.log(1 - x_L)\n",
    "\n",
    "\n",
    "def errorPerSample(X, yn):\n",
    "    return errorf(X[-1][-1], yn)\n",
    "\n",
    "\n",
    "def derivativeError(x_L, y):\n",
    "    if y == 1:\n",
    "        # derivative of np.log(x_L)\n",
    "        return 1 / (x_L)\n",
    "    else:\n",
    "        # derivative of -np.log(1 - x_L)\n",
    "        return 1 / (1 - x_L)\n",
    "\n",
    "\n",
    "def pred(x_n, weights):\n",
    "    x, s = forwardPropagation(x_n, weights)\n",
    "    res = 1 if x[-1][-1] >= 0.5 else -1\n",
    "    return res\n",
    "\n",
    "\n",
    "def confMatrix(X_train, y_train, w):\n",
    "    # Add implementation here\n",
    "\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "\n",
    "    y_pred = []\n",
    "    for x_n in X_train:\n",
    "        y_pred.append(pred(x_n, w))\n",
    "\n",
    "    # the confusion maxtrix that we will return\n",
    "    # matrix = [[0, 0], [0, 0]]\n",
    "    matrix = np.zeros((2, 2), np.int8)\n",
    "\n",
    "    # Populating our matrix using the prediction data\n",
    "    for index, y in enumerate(y_train):\n",
    "        if y == -1 and y_pred[index] == -1:\n",
    "            matrix[0][0] += 1\n",
    "        elif y == -1 and y_pred[index] == 1:\n",
    "            matrix[0][1] += 1\n",
    "        elif y == 1 and y_pred[index] == -1:\n",
    "            matrix[1][0] += 1\n",
    "        else:\n",
    "            matrix[1][1] += 1\n",
    "\n",
    "    # returning the result\n",
    "    return matrix\n",
    "    # return confusion_matrix(y_train, y_pred)\n",
    "\n",
    "\n",
    "def plotErr(e, epochs):\n",
    "    x_axis = range(1, epochs + 1)\n",
    "    plt.plot(e)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_SciKit(X_train, X_test, Y_train, Y_test):\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(300, 100), random_state=1, alpha=10 ** (-5))\n",
    "    nn.fit(X_train, Y_train)\n",
    "    pred = nn.predict(X_test)\n",
    "    cm = confusion_matrix(Y_test, pred)\n",
    "    return cm\n",
    "\n",
    "\n",
    "def test():\n",
    "    X_train, y_train = load_iris(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train[50:], y_train[50:], test_size=0.2)\n",
    "\n",
    "    for i in range(80):\n",
    "        if y_train[i] == 1:\n",
    "            y_train[i] = -1\n",
    "        else:\n",
    "            y_train[i] = 1\n",
    "    for j in range(20):\n",
    "        if y_test[j] == 1:\n",
    "            y_test[j] = -1\n",
    "        else:\n",
    "            y_test[j] = 1\n",
    "\n",
    "    err, w = fit_NeuralNetwork(X_train, y_train, 1e-2, [30, 10], 100)\n",
    "\n",
    "    # plotErr(err, 100)\n",
    "\n",
    "    cM = confMatrix(X_test, y_test, w)\n",
    "\n",
    "    # sciKit = test_SciKit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    print(\"Confusion Matrix is from Part 1a is:\\n\", cM)\n",
    "    # print(\"Confusion Matrix from Part 1b is:\\n\", sciKit)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X_train = [\n",
    "    [2],\n",
    "    [3],\n",
    "    [1],\n",
    "    [2],\n",
    "    [4],\n",
    "    \n",
    "] + [\n",
    "    [-2],\n",
    "    [-3],\n",
    "    [-1],\n",
    "    [-2],\n",
    "    [-4],\n",
    "]\n",
    "y_train = [\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "] + [\n",
    "    -1,\n",
    "    -1,\n",
    "    -1,\n",
    "    -1,\n",
    "    -1\n",
    "]\n",
    "\n",
    "# X_train = list(reversed(X_train))\n",
    "# y_train = list(reversed(y_train))\n",
    "\n",
    "err, w = fit_NeuralNetwork(X_train, y_train, 1e-3, [30, 10], 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 285
    }
   ],
   "source": [
    "pred([1, 100],w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now with the real data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "X_train, y_train = load_iris(return_X_y=True)\n",
    "X_train = X_train[:10]\n",
    "y_train = y_train[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "X_train, y_train = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train[50:], y_train[50:], test_size=0.2)\n",
    "\n",
    "for i in range(80):\n",
    "    if y_train[i] == 1:\n",
    "        y_train[i] = -1\n",
    "    else:\n",
    "        y_train[i] = 1\n",
    "for j in range(20):\n",
    "    if y_test[j] == 1:\n",
    "        y_test[j] = -1\n",
    "    else:\n",
    "        y_test[j] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "err, w = fit_NeuralNetwork(X_train, y_train, 1e-2, [30, 10], 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "[(array([5.5, 2.5, 4. , 1.3]), -1),\n (array([6.6, 2.9, 4.6, 1.3]), -1),\n (array([6. , 2.9, 4.5, 1.5]), -1),\n (array([6.1, 2.9, 4.7, 1.4]), -1),\n (array([6.6, 3. , 4.4, 1.4]), -1),\n (array([7.2, 3.6, 6.1, 2.5]), 1),\n (array([7.9, 3.8, 6.4, 2. ]), 1),\n (array([6.3, 2.5, 5. , 1.9]), 1),\n (array([5. , 2.3, 3.3, 1. ]), -1),\n (array([6.7, 3.3, 5.7, 2.5]), 1),\n (array([5.8, 2.7, 5.1, 1.9]), 1),\n (array([7.7, 2.8, 6.7, 2. ]), 1),\n (array([6.9, 3.1, 4.9, 1.5]), -1),\n (array([7.4, 2.8, 6.1, 1.9]), 1),\n (array([5.7, 3. , 4.2, 1.2]), -1),\n (array([5.5, 2.4, 3.8, 1.1]), -1),\n (array([6.5, 3. , 5.8, 2.2]), 1),\n (array([7.2, 3.2, 6. , 1.8]), 1),\n (array([6.8, 3. , 5.5, 2.1]), 1),\n (array([6.8, 2.8, 4.8, 1.4]), -1),\n (array([5.7, 2.6, 3.5, 1. ]), -1),\n (array([4.9, 2.4, 3.3, 1. ]), -1),\n (array([7.2, 3. , 5.8, 1.6]), 1),\n (array([6.2, 2.8, 4.8, 1.8]), 1),\n (array([5.8, 2.8, 5.1, 2.4]), 1),\n (array([6.3, 2.7, 4.9, 1.8]), 1),\n (array([6.4, 2.8, 5.6, 2.1]), 1),\n (array([5.6, 2.9, 3.6, 1.3]), -1),\n (array([5.9, 3. , 5.1, 1.8]), 1),\n (array([6. , 2.7, 5.1, 1.6]), -1),\n (array([6.4, 3.2, 5.3, 2.3]), 1),\n (array([6.9, 3.1, 5.1, 2.3]), 1),\n (array([6.2, 2.9, 4.3, 1.3]), -1),\n (array([6.7, 3.3, 5.7, 2.1]), 1),\n (array([6.1, 3. , 4.6, 1.4]), -1),\n (array([5.8, 2.7, 3.9, 1.2]), -1),\n (array([6.3, 2.8, 5.1, 1.5]), 1),\n (array([5.6, 3. , 4.1, 1.3]), -1),\n (array([5.7, 2.9, 4.2, 1.3]), -1),\n (array([6.9, 3.2, 5.7, 2.3]), 1),\n (array([5.5, 2.3, 4. , 1.3]), -1),\n (array([6.2, 3.4, 5.4, 2.3]), 1),\n (array([5.5, 2.6, 4.4, 1.2]), -1),\n (array([5.6, 2.5, 3.9, 1.1]), -1),\n (array([7.7, 3. , 6.1, 2.3]), 1),\n (array([4.9, 2.5, 4.5, 1.7]), 1),\n (array([6.3, 2.3, 4.4, 1.3]), -1),\n (array([6.3, 2.9, 5.6, 1.8]), 1),\n (array([6.4, 2.7, 5.3, 1.9]), 1),\n (array([5.7, 2.8, 4.5, 1.3]), -1),\n (array([6.8, 3.2, 5.9, 2.3]), 1),\n (array([5.7, 2.8, 4.1, 1.3]), -1),\n (array([6.5, 3.2, 5.1, 2. ]), 1),\n (array([7.6, 3. , 6.6, 2.1]), 1),\n (array([7.1, 3. , 5.9, 2.1]), 1),\n (array([6.7, 3.1, 4.4, 1.4]), -1),\n (array([5.1, 2.5, 3. , 1.1]), -1),\n (array([5. , 2. , 3.5, 1. ]), -1),\n (array([6.4, 2.9, 4.3, 1.3]), -1),\n (array([5.4, 3. , 4.5, 1.5]), -1),\n (array([6.4, 2.8, 5.6, 2.2]), 1),\n (array([6.4, 3.1, 5.5, 1.8]), 1),\n (array([6. , 3.4, 4.5, 1.6]), -1),\n (array([5.9, 3. , 4.2, 1.5]), -1),\n (array([5.6, 2.8, 4.9, 2. ]), 1),\n (array([6.5, 2.8, 4.6, 1.5]), -1),\n (array([5.2, 2.7, 3.9, 1.4]), -1),\n (array([5.8, 2.6, 4. , 1.2]), -1),\n (array([7.7, 3.8, 6.7, 2.2]), 1),\n (array([7.7, 2.6, 6.9, 2.3]), 1),\n (array([7. , 3.2, 4.7, 1.4]), -1),\n (array([6.4, 3.2, 4.5, 1.5]), -1),\n (array([5.8, 2.7, 4.1, 1. ]), -1),\n (array([6.1, 2.8, 4.7, 1.2]), -1),\n (array([5.5, 2.4, 3.7, 1. ]), -1),\n (array([6.5, 3. , 5.2, 2. ]), 1),\n (array([5.6, 2.7, 4.2, 1.3]), -1),\n (array([6.3, 3.4, 5.6, 2.4]), 1),\n (array([7.3, 2.9, 6.3, 1.8]), 1),\n (array([6.7, 3. , 5. , 1.7]), -1)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 224
    }
   ],
   "source": [
    "list(zip(X_train, y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 225
    }
   ],
   "source": [
    "pred([1, 5.6, 3. , 4.5, 1.5],w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "data": {
      "text/plain": "array([array([[0.10299327, 0.10299327, 0.10299327, 0.10299327, 0.10299327,\n        0.10299327, 0.10299327, 0.10299327, 0.10299327, 0.10299327,\n        0.10299327, 0.10299327, 0.10299327, 0.10299327, 0.10299327,\n        0.10299327, 0.10299327, 0.10299327, 0.10299327, 0.10299327,\n        0.10299327, 0.10299327, 0.10299327, 0.10299327, 0.10299327,\n        0.10299327, 0.10299327, 0.10299327, 0.10299327, 0.10299327],\n       [0.11700439, 0.11700439, 0.11700439, 0.11700439, 0.11700439,\n        0.11700439, 0.11700439, 0.11700439, 0.11700439, 0.11700439,\n        0.11700439, 0.11700439, 0.11700439, 0.11700439, 0.11700439,\n        0.11700439, 0.11700439, 0.11700439, 0.11700439, 0.11700439,\n        0.11700439, 0.11700439, 0.11700439, 0.11700439, 0.11700439,\n        0.11700439, 0.11700439, 0.11700439, 0.11700439, 0.11700439],\n       [0.10793743, 0.10793743, 0.10793743, 0.10793743, 0.10793743,\n        0.10793743, 0.10793743, 0.10793743, 0.10793743, 0.10793743,\n        0.10793743, 0.10793743, 0.10793743, 0.10793743, 0.10793743,\n        0.10793743, 0.10793743, 0.10793743, 0.10793743, 0.10793743,\n        0.10793743, 0.10793743, 0.10793743, 0.10793743, 0.10793743,\n        0.10793743, 0.10793743, 0.10793743, 0.10793743, 0.10793743],\n       [0.11237106, 0.11237106, 0.11237106, 0.11237106, 0.11237106,\n        0.11237106, 0.11237106, 0.11237106, 0.11237106, 0.11237106,\n        0.11237106, 0.11237106, 0.11237106, 0.11237106, 0.11237106,\n        0.11237106, 0.11237106, 0.11237106, 0.11237106, 0.11237106,\n        0.11237106, 0.11237106, 0.11237106, 0.11237106, 0.11237106,\n        0.11237106, 0.11237106, 0.11237106, 0.11237106, 0.11237106],\n       [0.10394832, 0.10394832, 0.10394832, 0.10394832, 0.10394832,\n        0.10394832, 0.10394832, 0.10394832, 0.10394832, 0.10394832,\n        0.10394832, 0.10394832, 0.10394832, 0.10394832, 0.10394832,\n        0.10394832, 0.10394832, 0.10394832, 0.10394832, 0.10394832,\n        0.10394832, 0.10394832, 0.10394832, 0.10394832, 0.10394832,\n        0.10394832, 0.10394832, 0.10394832, 0.10394832, 0.10394832]]),\n       array([[0.1029269 , 0.1029269 , 0.1029269 , 0.1029269 , 0.1029269 ,\n        0.1029269 , 0.1029269 , 0.1029269 , 0.1029269 , 0.1029269 ],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481],\n       [0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481,\n        0.10458481, 0.10458481, 0.10458481, 0.10458481, 0.10458481]]),\n       array([[0.1198479 ],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867],\n       [0.19688867]])], dtype=object)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 226
    }
   ],
   "source": [
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}